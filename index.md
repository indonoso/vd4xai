---
layout: single
sidebar:
  nav: "default"
---

In recent years, we have witnessed the rapid adoption of AI to automate processes and augment human capabilities in domains like medicine, security, and the military. However, AI-based systems often lack explainability and do not let users understand the rationale behind AI models' predictions. This problem has driven the development of explainable AI (XAI) and its various methods to construct effective explanations of these models. Several of those methods resort to proposing visualizations that support explanations. Consequently, this field of study also requires knowledge to design and implement appropriate visualizations.
Although there are workflows for designing interactive machine learning (IML) or XAI applications, they mainly focus on the stages of the machine learning (ML) model-building process, neglecting guidelines or strategies to design or analyze the visualizations intended for XAI applications. Therefore, we propose to bridge this gap starting from the XAI task space and connecting it to Munzner's widely adopted nested model for visualization design. Our goal is to bridge the gap between AI/ML experts, visualization designers, domain experts, and final users for building effective XAI visualizations. To achieve this goal, we developed <span class="project">VD4XAI</span>, a framework that connects visualization design with Explainable AI tasks. This structure aims to guide XAI visualizations' analysis and design process for local explanations. We ran a qualitative study to evaluate how people experience working with this process. We found that it helped participants feel more confident with their design choices and allowed them to consider and evaluate options systematically.
